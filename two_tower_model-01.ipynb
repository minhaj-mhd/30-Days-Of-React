{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2jLAn3axSqbEpdahBMHgH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhaj-mhd/30-Days-Of-React/blob/master/two_tower_model-01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6rQ19NkZo8s",
        "outputId": "430553f5-c67d-443e-9557-8fc0d8c44583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Installing and upgrading all required packages...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "âœ… All packages have been installed and upgraded.\n"
          ]
        }
      ],
      "source": [
        "print(\"â³ Installing and upgrading all required packages...\")\n",
        "\n",
        "%pip install --upgrade -q tensorflow tensorflow-recommenders tf-keras tensorflow-text\n",
        "%pip install -q faiss-cpu\n",
        "\n",
        "print(\"\\nâœ… All packages have been installed and upgraded.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade -q tensorflow-decision-forests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbdXsqBCabJ4",
        "outputId": "c281b5d3-70e2-4669-be7b-c39fb92ae4ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "import tf_keras\n",
        "import faiss\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"tensorflow-recommenders: {tfrs.__version__}\")\n",
        "print(f\"tf-keras: {tf_keras.__version__}\")\n",
        "print(f\"faiss-cpu: {faiss.__version__}\")\n",
        "print(f\"tensorflow-text: {tf_text.__version__}\")\n",
        "print(f\"tensorflow-decision-forests: {tfdf.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "_roixOESalk5",
        "outputId": "9cb1525c-b475-4044-9df2-c7a2c38ed2d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<p style=\"margin:0px;\">ğŸŒ² Try <a href=\"https://ydf.readthedocs.io/en/latest/\" target=\"_blank\">YDF</a>, the successor of\n",
              "    <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow\n",
              "        Decision Forests</a> using the same algorithms but with more features and faster\n",
              "    training!\n",
              "</p>\n",
              "<div style=\"display: flex; flex-wrap: wrap; margin:5px;max-width: 880px;\">\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            Old code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import tensorflow_decision_forests as tfdf\n",
              "\n",
              "tf_ds = tfdf.keras.pd_dataframe_to_tf_dataset(ds, label=\"l\")\n",
              "model = tfdf.keras.RandomForestModel(label=\"l\")\n",
              "model.fit(tf_ds)\n",
              "</pre>\n",
              "    </div>\n",
              "    <div style=\"width: 5px;\"></div>\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            New code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import ydf\n",
              "\n",
              "model = ydf.RandomForestLearner(label=\"l\").train(ds)\n",
              "</pre>\n",
              "    </div>\n",
              "</div>\n",
              "<p style=\"margin:0px;font-size: 9pt;\">(Learn more in the <a\n",
              "        href=\"https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/\" target=\"_blank\">migration\n",
              "        guide</a>)</p>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow: 2.19.0\n",
            "tensorflow-recommenders: v0.7.3\n",
            "tf-keras: 2.19.0\n",
            "faiss-cpu: 1.11.0\n",
            "tensorflow-text: 2.19.0\n",
            "tensorflow-decision-forests: 1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import faiss\n",
        "\n",
        "# --- Step 1: Fabricate Data ---\n",
        "print(\"[1] Fabricating data...\")\n",
        "categories = [\"gadget\", \"apparel\", \"book\", \"tool\", \"toy\", \"utensil\"]\n",
        "num_items = 5000\n",
        "item_titles = [f\"Product {i}\" for i in range(num_items)]\n",
        "description_templates = [\n",
        "    lambda i: f\"High-quality, durable {categories[i % len(categories)]} for all your needs. Model v{i % 10}. Made from premium materials.\",\n",
        "    lambda i: f\"An affordable and reliable {categories[i % len(categories)]}. Perfect for beginners. Item #{i}.\",\n",
        "    lambda i: f\"The ultimate professional-grade {categories[i % len(categories)]}. Features advanced technology. SKU {i}.\",\n",
        "]\n",
        "item_descriptions = [description_templates[i % 3](i) for i in range(num_items)]\n",
        "items_data = {\n",
        "    \"item_id\": [str(i) for i in range(num_items)],\n",
        "    \"item_title\": item_titles,\n",
        "    \"item_description\": item_descriptions\n",
        "}\n",
        "items_df = pd.DataFrame(items_data)\n",
        "\n",
        "num_users = 500\n",
        "num_interactions = 20000\n",
        "users_data = {\n",
        "    \"user_id\": [str(np.random.randint(0, num_users)) for _ in range(num_interactions)],\n",
        "    \"item_id\": [str(np.random.randint(0, num_items)) for _ in range(num_interactions)],\n",
        "}\n",
        "interactions_df = pd.DataFrame(users_data)\n",
        "print(f\"Generated {len(items_df)} items and {len(interactions_df)} interactions.\")\n",
        "items_ds = tf.data.Dataset.from_tensor_slices(dict(items_df))\n",
        "\n",
        "# --- Step 2: Self-Supervised Item Tower ---\n",
        "print(\"\\n[2] Building and training the self-supervised Item Tower...\")\n",
        "embedding_dimension = 32\n",
        "max_tokens = 10_000\n",
        "sequence_length = 100\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=sequence_length)\n",
        "text_vectorizer.adapt(items_ds.map(lambda x: x[\"item_description\"]).batch(128))\n",
        "\n",
        "class ItemModel(tf.keras.Model):\n",
        "    def __init__(self, vectorizer):\n",
        "        super().__init__()\n",
        "        self.vectorizer = vectorizer\n",
        "        self.embedding = tf.keras.Sequential([\n",
        "            self.vectorizer,\n",
        "            tf.keras.layers.Embedding(input_dim=self.vectorizer.vocabulary_size(), output_dim=embedding_dimension, mask_zero=True),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ])\n",
        "        self.dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embedding_dimension)\n",
        "        ])\n",
        "    def call(self, inputs):\n",
        "        return self.dense(self.embedding(inputs[\"item_description\"]))\n",
        "\n",
        "class SelfSupervisedItemTwoTower(tfrs.Model):\n",
        "    def __init__(self, item_model):\n",
        "        super().__init__()\n",
        "        self.item_model = item_model\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "    def compute_loss(self, features, training=False):\n",
        "        item_embeddings = self.item_model(features)\n",
        "        return self.task(query_embeddings=item_embeddings, candidate_embeddings=item_embeddings)\n",
        "\n",
        "item_tower = ItemModel(text_vectorizer)\n",
        "item_model_trainer = SelfSupervisedItemTwoTower(item_tower)\n",
        "item_model_trainer.compile(optimizer=tf.keras.optimizers.Adagrad(0.05))\n",
        "train_item_ds = items_ds.map(lambda x: {\"item_description\": x[\"item_description\"]}).batch(256).cache()\n",
        "item_model_trainer.fit(train_item_ds, epochs=5)\n",
        "print(\"Item Tower training complete.\")\n",
        "\n",
        "# --- Step 3: Generate and Store Item Embeddings in Faiss ---\n",
        "print(\"\\n[3] Generating item embeddings and storing in Faiss...\")\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "item_embeddings_generator = items_ds.batch(256).map(lambda x: item_tower(x))\n",
        "all_item_embeddings = np.concatenate(list(item_embeddings_generator.as_numpy_iterator()))\n",
        "index.add(all_item_embeddings)\n",
        "print(f\"Faiss index now contains {index.ntotal} vectors.\")\n",
        "index_to_item_id = {i: item_id for i, item_id in enumerate(items_df[\"item_id\"])}\n",
        "\n",
        "# --- Step 4: Train the User Tower (FIXED) ---\n",
        "print(\"\\n[4] Building and training the User Tower...\")\n",
        "unique_user_ids = interactions_df[\"user_id\"].unique()\n",
        "\n",
        "class UserModel(tf.keras.Model):\n",
        "    def __init__(self, user_ids):\n",
        "        super().__init__()\n",
        "        self.user_embedding = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=user_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(user_ids) + 1, embedding_dimension)\n",
        "        ])\n",
        "        self.dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embedding_dimension)\n",
        "        ])\n",
        "    def call(self, inputs):\n",
        "        return self.dense(self.user_embedding(inputs))\n",
        "\n",
        "class UserItemRetrievalModel(tfrs.Model):\n",
        "    def __init__(self, user_model, item_model, items_ds):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.item_model = item_model\n",
        "        self.item_model.trainable = False  # Keep item model frozen\n",
        "\n",
        "        # For FactorizedTopK, we need to provide a way to compute candidate embeddings\n",
        "        # Let's create a simple retrieval task without the complex metrics for now\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def compute_loss(self, data, training=False):\n",
        "        user_embeddings = self.user_model(data[\"user_id\"])\n",
        "\n",
        "        # Get item embeddings for the interacted items\n",
        "        item_data = {\"item_description\": data[\"item_description\"]}\n",
        "        item_embeddings = self.item_model(item_data)\n",
        "\n",
        "        return self.task(\n",
        "            query_embeddings=user_embeddings,\n",
        "            candidate_embeddings=item_embeddings\n",
        "        )\n",
        "\n",
        "# Prepare training data with item descriptions\n",
        "interactions_with_desc_df = pd.merge(interactions_df, items_df[['item_id', 'item_description']], on='item_id')\n",
        "full_interactions_ds = tf.data.Dataset.from_tensor_slices(dict(interactions_with_desc_df))\n",
        "train_ds_user = full_interactions_ds.shuffle(10_000).batch(256).cache()\n",
        "\n",
        "user_tower = UserModel(unique_user_ids)\n",
        "user_model_trainer = UserItemRetrievalModel(user_tower, item_tower, items_ds)\n",
        "user_model_trainer.compile(optimizer=tf.keras.optimizers.Adagrad(0.05))\n",
        "\n",
        "# Train the user model\n",
        "user_model_trainer.fit(train_ds_user, epochs=5)\n",
        "print(\"User Tower training complete.\")\n",
        "\n",
        "# --- Step 5: Serve Recommendations ---\n",
        "print(\"\\n[5] Serving recommendations...\")\n",
        "def get_recommendations(user_id, top_k=5):\n",
        "    print(f\"\\n--- Getting recommendations for user '{user_id}' ---\")\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        random_user = np.random.choice(unique_user_ids)\n",
        "        print(f\"Simulating recommendations based on a similar user: '{random_user}'\")\n",
        "        user_id = random_user\n",
        "\n",
        "    user_embedding = user_tower(tf.constant([user_id])).numpy()\n",
        "    distances, indices = index.search(user_embedding, top_k)\n",
        "    print(f\"Top {top_k} recommendations:\")\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        item_id = index_to_item_id[idx]\n",
        "        item_title = items_df[items_df['item_id'] == item_id]['item_title'].values[0]\n",
        "        print(f\"  {i+1}. Item ID: {item_id} | Title: '{item_title}' (Distance: {distances[0][i]:.4f})\")\n",
        "\n",
        "get_recommendations(\"10\")\n",
        "get_recommendations(\"123\")\n",
        "get_recommendations(\"9999\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlwTsIUjaz4S",
        "outputId": "89e65459-8a31-4538-bca9-bfd107722a1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Fabricating data...\n",
            "Generated 5000 items and 20000 interactions.\n",
            "\n",
            "[2] Building and training the self-supervised Item Tower...\n",
            "Epoch 1/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1157.9170 - regularization_loss: 0.0000e+00 - total_loss: 1157.9170\n",
            "Epoch 2/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1041.7927 - regularization_loss: 0.0000e+00 - total_loss: 1041.7927\n",
            "Epoch 3/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 998.8961 - regularization_loss: 0.0000e+00 - total_loss: 998.8961  \n",
            "Epoch 4/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 913.1841 - regularization_loss: 0.0000e+00 - total_loss: 913.1841\n",
            "Epoch 5/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 909.8185 - regularization_loss: 0.0000e+00 - total_loss: 909.8185\n",
            "Item Tower training complete.\n",
            "\n",
            "[3] Generating item embeddings and storing in Faiss...\n",
            "Faiss index now contains 5000 vectors.\n",
            "\n",
            "[4] Building and training the User Tower...\n",
            "Epoch 1/5\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 1391.6023 - regularization_loss: 0.0000e+00 - total_loss: 1391.6023\n",
            "Epoch 2/5\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1382.9640 - regularization_loss: 0.0000e+00 - total_loss: 1382.9640\n",
            "Epoch 3/5\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1378.9749 - regularization_loss: 0.0000e+00 - total_loss: 1378.9749\n",
            "Epoch 4/5\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1376.7136 - regularization_loss: 0.0000e+00 - total_loss: 1376.7136\n",
            "Epoch 5/5\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1375.2988 - regularization_loss: 0.0000e+00 - total_loss: 1375.2988\n",
            "User Tower training complete.\n",
            "\n",
            "[5] Serving recommendations...\n",
            "\n",
            "--- Getting recommendations for user '10' ---\n",
            "Top 5 recommendations:\n",
            "  1. Item ID: 6 | Title: 'Product 6' (Distance: 11.7209)\n",
            "  2. Item ID: 36 | Title: 'Product 36' (Distance: 11.7209)\n",
            "  3. Item ID: 66 | Title: 'Product 66' (Distance: 11.7209)\n",
            "  4. Item ID: 96 | Title: 'Product 96' (Distance: 11.7209)\n",
            "  5. Item ID: 126 | Title: 'Product 126' (Distance: 11.7209)\n",
            "\n",
            "--- Getting recommendations for user '123' ---\n",
            "Top 5 recommendations:\n",
            "  1. Item ID: 3331 | Title: 'Product 3331' (Distance: 11.8699)\n",
            "  2. Item ID: 3445 | Title: 'Product 3445' (Distance: 11.8854)\n",
            "  3. Item ID: 1129 | Title: 'Product 1129' (Distance: 11.9183)\n",
            "  4. Item ID: 3409 | Title: 'Product 3409' (Distance: 11.9218)\n",
            "  5. Item ID: 3535 | Title: 'Product 3535' (Distance: 11.9220)\n",
            "\n",
            "--- Getting recommendations for user '9999' ---\n",
            "User '9999' is a new user (cold start).\n",
            "Simulating recommendations based on a similar user: '26'\n",
            "Top 5 recommendations:\n",
            "  1. Item ID: 6 | Title: 'Product 6' (Distance: 11.3967)\n",
            "  2. Item ID: 36 | Title: 'Product 36' (Distance: 11.3967)\n",
            "  3. Item ID: 66 | Title: 'Product 66' (Distance: 11.3967)\n",
            "  4. Item ID: 96 | Title: 'Product 96' (Distance: 11.3967)\n",
            "  5. Item ID: 126 | Title: 'Product 126' (Distance: 11.3967)\n"
          ]
        }
      ]
    }
  ]
}