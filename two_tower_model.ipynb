{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoGdkf4e7/b0mdx+bjOPW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhaj-mhd/30-Days-Of-React/blob/master/two_tower_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6rQ19NkZo8s",
        "outputId": "430553f5-c67d-443e-9557-8fc0d8c44583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Installing and upgrading all required packages...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "✅ All packages have been installed and upgraded.\n"
          ]
        }
      ],
      "source": [
        "print(\"⏳ Installing and upgrading all required packages...\")\n",
        "\n",
        "%pip install --upgrade -q tensorflow tensorflow-recommenders tf-keras tensorflow-text\n",
        "%pip install -q faiss-cpu\n",
        "\n",
        "print(\"\\n✅ All packages have been installed and upgraded.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade -q tensorflow-decision-forests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbdXsqBCabJ4",
        "outputId": "c281b5d3-70e2-4669-be7b-c39fb92ae4ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "import tf_keras\n",
        "import faiss\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"tensorflow-recommenders: {tfrs.__version__}\")\n",
        "print(f\"tf-keras: {tf_keras.__version__}\")\n",
        "print(f\"faiss-cpu: {faiss.__version__}\")\n",
        "print(f\"tensorflow-text: {tf_text.__version__}\")\n",
        "print(f\"tensorflow-decision-forests: {tfdf.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "_roixOESalk5",
        "outputId": "9cb1525c-b475-4044-9df2-c7a2c38ed2d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<p style=\"margin:0px;\">🌲 Try <a href=\"https://ydf.readthedocs.io/en/latest/\" target=\"_blank\">YDF</a>, the successor of\n",
              "    <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow\n",
              "        Decision Forests</a> using the same algorithms but with more features and faster\n",
              "    training!\n",
              "</p>\n",
              "<div style=\"display: flex; flex-wrap: wrap; margin:5px;max-width: 880px;\">\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            Old code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import tensorflow_decision_forests as tfdf\n",
              "\n",
              "tf_ds = tfdf.keras.pd_dataframe_to_tf_dataset(ds, label=\"l\")\n",
              "model = tfdf.keras.RandomForestModel(label=\"l\")\n",
              "model.fit(tf_ds)\n",
              "</pre>\n",
              "    </div>\n",
              "    <div style=\"width: 5px;\"></div>\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            New code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import ydf\n",
              "\n",
              "model = ydf.RandomForestLearner(label=\"l\").train(ds)\n",
              "</pre>\n",
              "    </div>\n",
              "</div>\n",
              "<p style=\"margin:0px;font-size: 9pt;\">(Learn more in the <a\n",
              "        href=\"https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/\" target=\"_blank\">migration\n",
              "        guide</a>)</p>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow: 2.19.0\n",
            "tensorflow-recommenders: v0.7.3\n",
            "tf-keras: 2.19.0\n",
            "faiss-cpu: 1.11.0\n",
            "tensorflow-text: 2.19.0\n",
            "tensorflow-decision-forests: 1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import faiss\n",
        "\n",
        "# --- Step 1: Fabricate Data ---\n",
        "print(\"[1] Fabricating data...\")\n",
        "categories = [\"gadget\", \"apparel\", \"book\", \"tool\", \"toy\", \"utensil\"]\n",
        "num_items = 5000\n",
        "item_titles = [f\"Product {i}\" for i in range(num_items)]\n",
        "description_templates = [\n",
        "    lambda i: f\"High-quality, durable {categories[i % len(categories)]} for all your needs. Model v{i % 10}. Made from premium materials.\",\n",
        "    lambda i: f\"An affordable and reliable {categories[i % len(categories)]}. Perfect for beginners. Item #{i}.\",\n",
        "    lambda i: f\"The ultimate professional-grade {categories[i % len(categories)]}. Features advanced technology. SKU {i}.\",\n",
        "]\n",
        "item_descriptions = [description_templates[i % 3](i) for i in range(num_items)]\n",
        "items_data = {\n",
        "    \"item_id\": [str(i) for i in range(num_items)],\n",
        "    \"item_title\": item_titles,\n",
        "    \"item_description\": item_descriptions\n",
        "}\n",
        "items_df = pd.DataFrame(items_data)\n",
        "\n",
        "num_users = 500\n",
        "num_interactions = 20000\n",
        "users_data = {\n",
        "    \"user_id\": [str(np.random.randint(0, num_users)) for _ in range(num_interactions)],\n",
        "    \"item_id\": [str(np.random.randint(0, num_items)) for _ in range(num_interactions)],\n",
        "}\n",
        "interactions_df = pd.DataFrame(users_data)\n",
        "print(f\"Generated {len(items_df)} items and {len(interactions_df)} interactions.\")\n",
        "items_ds = tf.data.Dataset.from_tensor_slices(dict(items_df))\n",
        "\n",
        "# --- Step 2: Self-Supervised Item Tower ---\n",
        "print(\"\\n[2] Building and training the self-supervised Item Tower...\")\n",
        "embedding_dimension = 32\n",
        "max_tokens = 10_000\n",
        "sequence_length = 100\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=sequence_length)\n",
        "text_vectorizer.adapt(items_ds.map(lambda x: x[\"item_description\"]).batch(128))\n",
        "\n",
        "class ItemModel(tf.keras.Model):\n",
        "    def __init__(self, vectorizer):\n",
        "        super().__init__()\n",
        "        self.vectorizer = vectorizer\n",
        "        self.embedding = tf.keras.Sequential([\n",
        "            self.vectorizer,\n",
        "            tf.keras.layers.Embedding(input_dim=self.vectorizer.vocabulary_size(), output_dim=embedding_dimension, mask_zero=True),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ])\n",
        "        self.dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embedding_dimension)\n",
        "        ])\n",
        "    def call(self, inputs):\n",
        "        return self.dense(self.embedding(inputs[\"item_description\"]))\n",
        "\n",
        "class SelfSupervisedItemTwoTower(tfrs.Model):\n",
        "    def __init__(self, item_model):\n",
        "        super().__init__()\n",
        "        self.item_model = item_model\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "    def compute_loss(self, features, training=False):\n",
        "        item_embeddings = self.item_model(features)\n",
        "        return self.task(query_embeddings=item_embeddings, candidate_embeddings=item_embeddings)\n",
        "\n",
        "item_tower = ItemModel(text_vectorizer)\n",
        "item_model_trainer = SelfSupervisedItemTwoTower(item_tower)\n",
        "item_model_trainer.compile(optimizer=tf.keras.optimizers.Adagrad(0.05))\n",
        "train_item_ds = items_ds.map(lambda x: {\"item_description\": x[\"item_description\"]}).batch(256).cache()\n",
        "item_model_trainer.fit(train_item_ds, epochs=5)\n",
        "print(\"Item Tower training complete.\")\n",
        "\n",
        "# --- Step 3: Generate and Store Item Embeddings in Faiss ---\n",
        "print(\"\\n[3] Generating item embeddings and storing in Faiss...\")\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "item_embeddings_generator = items_ds.batch(256).map(lambda x: item_tower(x))\n",
        "all_item_embeddings = np.concatenate(list(item_embeddings_generator.as_numpy_iterator()))\n",
        "index.add(all_item_embeddings)\n",
        "print(f\"Faiss index now contains {index.ntotal} vectors.\")\n",
        "index_to_item_id = {i: item_id for i, item_id in enumerate(items_df[\"item_id\"])}\n",
        "\n",
        "# --- Step 4: Train the User Tower ---\n",
        "print(\"\\n[4] Building and training the User Tower...\")\n",
        "unique_user_ids = np.unique(interactions_df[\"user_id\"])\n",
        "\n",
        "# Pre-compute all item embeddings to use as candidates for metrics\n",
        "print(\"Pre-computing all item embeddings into a single tensor for the candidate set...\")\n",
        "# Convert to NumPy array before passing to FactorizedTopK\n",
        "all_item_embeddings_np = np.concatenate(list(items_ds.batch(256).map(item_tower).as_numpy_iterator()))\n",
        "print(f\"Candidate embeddings numpy array shape: {all_item_embeddings_np.shape}\")\n",
        "item_ids_tensor = tf.constant(item_titles)\n",
        "item_embeddings_tensor = tf.convert_to_tensor(all_item_embeddings_np, dtype=tf.float32)\n",
        "candidate_dataset = items_ds.map(lambda x: (x[\"item_id\"], {\"item_description\": x[\"item_description\"]}))\n",
        "class UserModel(tf.keras.Model):\n",
        "    def __init__(self, user_ids):\n",
        "        super().__init__()\n",
        "        self.user_embedding = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=user_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(user_ids) + 1, embedding_dimension)\n",
        "        ])\n",
        "        self.dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embedding_dimension)\n",
        "        ])\n",
        "    def call(self, inputs):\n",
        "        return self.dense(self.user_embedding(inputs))\n",
        "\n",
        "class UserItemRetrievalModel(tfrs.Model):\n",
        "    def __init__(self, user_model, item_model, candidate_dataset):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.item_model = item_model\n",
        "        self.item_model.trainable = False\n",
        "\n",
        "        # Pass the pre-computed item embeddings directly to FactorizedTopK\n",
        "        self.task = tfrs.tasks.Retrieval(\n",
        "            metrics=tfrs.metrics.FactorizedTopK(candidates=candidate_dataset,\n",
        "                                                ks=[1, 5, 10])\n",
        "        )\n",
        "\n",
        "    def compute_loss(self, data, training=False):\n",
        "        user_embeddings = self.user_model(data[\"user_id\"])\n",
        "        item_embeddings = self.item_model({\"item_description\": data[\"item_description\"]})\n",
        "        # Pass the true item embeddings and IDs from the batch to the task for loss and metric calculation\n",
        "        return self.task(\n",
        "            query_embeddings=user_embeddings,\n",
        "            candidate_embeddings=item_embeddings,\n",
        "            candidate_ids=data[\"item_id\"]\n",
        "        )\n",
        "\n",
        "\n",
        "interactions_with_desc_df = pd.merge(interactions_df, items_df[['item_id', 'item_description']], on='item_id')\n",
        "full_interactions_ds = tf.data.Dataset.from_tensor_slices(dict(interactions_with_desc_df))\n",
        "train_ds_user = full_interactions_ds.shuffle(10_000).batch(256).cache()\n",
        "\n",
        "user_tower = UserModel(unique_user_ids)\n",
        "# Pass the pre-computed item embeddings (NumPy array) for metrics\n",
        "user_model_trainer = UserItemRetrievalModel(user_tower, item_tower, candidate_dataset)\n",
        "user_model_trainer.compile(optimizer=tf.keras.optimizers.Adagrad(0.05))\n",
        "user_model_trainer.fit(train_ds_user, epochs=5)\n",
        "print(\"User Tower training complete.\")\n",
        "\n",
        "# --- Step 5: Serve Recommendations ---\n",
        "print(\"\\n[5] Serving recommendations...\")\n",
        "def get_recommendations(user_id, top_k=5):\n",
        "    print(f\"\\n--- Getting recommendations for user '{user_id}' ---\")\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        random_user = np.random.choice(unique_user_ids)\n",
        "        print(f\"Simulating recommendations based on a similar user: '{random_user}'\")\n",
        "        user_id = random_user\n",
        "\n",
        "    user_embedding = user_tower(tf.constant([user_id])).numpy()\n",
        "    distances, indices = index.search(user_embedding, top_k)\n",
        "    print(f\"Top {top_k} recommendations:\")\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        item_id = index_to_item_id[idx]\n",
        "        item_title = items_df[items_df['item_id'] == item_id]['item_title'].values[0]\n",
        "        print(f\"  {i+1}. Item ID: {item_id} | Title: '{item_title}' (Distance: {distances[0][i]:.4f})\")\n",
        "\n",
        "get_recommendations(\"10\")\n",
        "get_recommendations(\"123\")\n",
        "get_recommendations(\"9999\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "jlwTsIUjaz4S",
        "outputId": "2ac4d0a4-5717-4122-d08b-1fc4a5fb48e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Fabricating data...\n",
            "Generated 5000 items and 20000 interactions.\n",
            "\n",
            "[2] Building and training the self-supervised Item Tower...\n",
            "Epoch 1/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1200.0494 - regularization_loss: 0.0000e+00 - total_loss: 1200.0494\n",
            "Epoch 2/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1047.5115 - regularization_loss: 0.0000e+00 - total_loss: 1047.5115\n",
            "Epoch 3/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 975.6721 - regularization_loss: 0.0000e+00 - total_loss: 975.6721  \n",
            "Epoch 4/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 963.3029 - regularization_loss: 0.0000e+00 - total_loss: 963.3029  \n",
            "Epoch 5/5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 962.1260 - regularization_loss: 0.0000e+00 - total_loss: 962.1260\n",
            "Item Tower training complete.\n",
            "\n",
            "[3] Generating item embeddings and storing in Faiss...\n",
            "Faiss index now contains 5000 vectors.\n",
            "\n",
            "[4] Building and training the User Tower...\n",
            "Pre-computing all item embeddings into a single tensor for the candidate set...\n",
            "Candidate embeddings numpy array shape: (5000, 32)\n",
            "candidate_dataset <_MapDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), {'item_description': TensorSpec(shape=(), dtype=tf.string, name=None)})>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot convert '('c', 'o', 'u', 'n', 't', 'e', 'r')' to a shape. Found invalid entry 'c' of type '<class 'str'>'. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2861931372.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0muser_tower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_user_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m# Pass the pre-computed item embeddings (NumPy array) for metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0muser_model_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserItemRetrievalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_tower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0muser_model_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0muser_model_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-2861931372.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, user_model, item_model, candidate_dataset)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Pass the pre-computed item embeddings directly to FactorizedTopK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         self.task = tfrs.tasks.Retrieval(\n\u001b[0;32m--> 122\u001b[0;31m             metrics=tfrs.metrics.FactorizedTopK(candidates=candidate_dataset,\n\u001b[0m\u001b[1;32m    123\u001b[0m                                                 ks=[1, 5, 10])\n\u001b[1;32m    124\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_recommenders/metrics/factorized_top_k.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, candidates, ks, name)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       candidates = (\n\u001b[0;32m---> 79\u001b[0;31m           \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorized_top_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreaming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mindex_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_recommenders/layers/factorized_top_k.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, query_model, k, handle_incomplete_batches, num_parallel_calls, sorted_order)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m   def index_from_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, shape, initializer, dtype, trainable, autocast, regularizer, constraint, aggregation, name)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             variable = backend.Variable(\n\u001b[0m\u001b[1;32m    545\u001b[0m                 \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initializer, shape, dtype, trainable, autocast, aggregation, name)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_with_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36m_validate_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36mstandardize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_int_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    583\u001b[0m                 \u001b[0;34mf\"Cannot convert '{shape}' to a shape. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                 \u001b[0;34mf\"Found invalid entry '{e}' of type '{type(e)}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot convert '('c', 'o', 'u', 'n', 't', 'e', 'r')' to a shape. Found invalid entry 'c' of type '<class 'str'>'. "
          ]
        }
      ]
    }
  ]
}